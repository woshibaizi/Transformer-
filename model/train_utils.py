import torch
import torch.nn as nn
from torch.autograd import Variable

class MultiGPULossCompute:
    def __init__(self,generator,criterion,devices,opt=None,chunk_size=5):
        '''
        初始化多GPU训练中的损失计算类
        :param generator:生成器模型,通常侍生成预测分布的网络
        :param criterion:损失函数,用于计算预测和目标之间的差距
        :param devices:使用的GPU设备列表
        :param opt:优化器对象,进行参数更新
        :param chunk_size:将数据分割成多个小块进行计算的大小
        '''
        self.generator = generator
        self.criterion = criterion
        self.opt = opt    #优化器
        self.devices = devices    #设备列表,包含多个gpu的id
        self.chunk_size = chunk_size  #每次计算的数据块大小

    def __call__(self,out,targets,normalize):
        '''
        进行多GPU的损失计算和训练
        :param out:模型输出
        :param targets:目标数据(真实标签)
        :param normalize:用于规范化损失的常数
        :return:总损失值(乘以normalize

        '''
        # 将模型输出通过生成器得到词表维度的分布
        logits = self.generator(out)
        loss = self.criterion(
            logits.contiguous().view(-1, logits.size(-1)),
            targets.contiguous().view(-1)
        )

        if self.opt is not None:
            # 反向传播并更新参数
            self.opt.optimizer.zero_grad()
            loss.backward()
            self.opt.step()
            self.opt.optimizer.zero_grad()

        # 返回标量损失值（未归一化），run_epoch 会用 ntokens 做平均
        return loss.item()

class NoamOpt:
    def __init__(self,model_size,factor,warmup,optimizer):
        """
        初始化优化器包装类
        :param model_size:模型大小,通常是d_model的大小,用于计算学习率
        :param factor:计算学习率的因子,(通常侍学习率的初始值)
        :param warmup:预热步数,决定学习率从较小值到较大值的增长速度
        :param optimizer:实际使用的优化器(例如Adam,SGD)
        """
        self.optimizer=optimizer        #存储实际使用的优化器
        self._step=0    #当前的训练步数
        self.warmup=warmup  #预热步数
        self.factor=factor  #学习率的因子
        self.model_size=model_size #模型的大小(通常是d_model,决定学习率的尺度)
        self._rate=0        #当前的学习率

    def step(self):
        '''更新优化器的参数和学习率'''
        self._step+=1   #增加当前的步数
        rate=self.rate()    #计算当前的学习率
        #更新优化器中所有参数的学习率
        for p in self.optimizer.param_groups:
            p['lr']=rate    #设置当前学习率
        self._rate=rate     #更新学习率
        self.optimizer.step()   #执行一次优化步骤(更新参数)

    def rate(self,step=None):
        """根据当前步数计算学习率"""
        # 如果没有传入step,使用当前步数
        if step is None:
            step=self._step
        #学习率计算公式:factor*(model_size ** -0.5)*min(step ** -0.5,step*warmup **-1.5)
        return self.factor*(self.model_size ** (-0.5)*min(step ** -0.5,step*self.warmup **(-1.5)))

def get_std_opt(model):
    #创建并返回一个NoamOpt优化器,包含Adam优化器作为基础
    return NoamOpt(model.src_embed[0].d_model,1,10000,
                   torch.optim.Adam(model.parameters(),lr=0,betas=(0.9,0.98),eps=1e-9))










